import pandas as pd
import pyarrow.parquet as pq
import os
from pathlib import Path
from tqdm import tqdm

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸
input_csv = Path("/tmp/synthetic_iris_million.csv")
output_dir = Path("/Users/dima/Apache-airflow/iris_parquet_parts")
num_parts = 20

# Ğ§Ñ‚ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ CSV
print(f"ğŸ“¥ Loading CSV: {input_csv}")
df = pd.read_csv(input_csv)
print(f"âœ… Loaded: {len(df):,} rows")

# ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ¿Ğ°Ğ¿ĞºĞ¸
output_dir.mkdir(parents=True, exist_ok=True)

# Ğ Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
rows_per_part = len(df) // num_parts
print(f"ğŸª“ Splitting into {num_parts} parts...")

for i in tqdm(range(num_parts), desc="ğŸ“¤ Writing Parquet"):
    start = i * rows_per_part
    end = None if i == num_parts - 1 else (i + 1) * rows_per_part
    chunk = df.iloc[start:end]
    
    filename = f"train-{i:05d}-of-{num_parts:05d}.parquet"
    chunk.to_parquet(output_dir / filename, index=False, engine="pyarrow")

print(f"\nâœ… Split complete: {num_parts} files saved in {output_dir}\n")

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
print("ğŸ” Validating parquet files...")
files = sorted(output_dir.glob("*.parquet"))
total_rows = 0
first_schema = pq.read_schema(files[0])

for f in tqdm(files, desc="ğŸ“„ Validating"):
    try:
        df_part = pd.read_parquet(f)
        total_rows += len(df_part)
        current_schema = pq.read_schema(f)
        assert current_schema == first_schema, f"âŒ Schema mismatch in {f.name}"
        print(f"âœ… OK: {f.name} â€” shape={df_part.shape}")
    except Exception as e:
        print(f"âŒ Error reading {f.name}: {e}")

print(f"\nğŸ“Š Total rows across all parts: {total_rows:,}")
print("âœ… All schemas match.")
